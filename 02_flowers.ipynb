{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             file_path   Label\n",
      "0       flower_photos/roses/16209331331_343c899d38.jpg   roses\n",
      "1        flower_photos/roses/5777669976_a205f61e5b.jpg   roses\n",
      "2      flower_photos/roses/4860145119_b1c3cbaa4e_n.jpg   roses\n",
      "3       flower_photos/roses/15011625580_7974c44bce.jpg   roses\n",
      "4     flower_photos/roses/17953368844_be3d18cf30_m.jpg   roses\n",
      "...                                                ...     ...\n",
      "3665     flower_photos/tulips/134143359_71fa8dd9a4.jpg  tulips\n",
      "3666    flower_photos/tulips/3637371174_a8dfcc1b35.jpg  tulips\n",
      "3667  flower_photos/tulips/6948239566_0ac0a124ee_n.jpg  tulips\n",
      "3668    flower_photos/tulips/2834890466_1cf220fba1.jpg  tulips\n",
      "3669   flower_photos/tulips/13953090784_0c7d7a904e.jpg  tulips\n",
      "\n",
      "[3670 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for dir, x, files in os.walk(\"flower_photos\"):\n",
    "    label=dir.split('/')[-1]\n",
    "    #print(dir)\n",
    "    #print(files)\n",
    "    for file in files:\n",
    "        path=os.path.join(dir, file)\n",
    "        data.append([path, label])\n",
    "df=pd.DataFrame(data, columns=[\"file_path\", \"Label\"])    \n",
    "#print(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2936 validated image filenames belonging to 5 classes.\n",
      "Found 734 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = datagen.flow_from_dataframe(\n",
    "    dataframe = df,\n",
    "    x_col = 'file_path',\n",
    "    y_col = 'Label',\n",
    "    target_size = (224,224),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "valid_gen = datagen.flow_from_dataframe(\n",
    "    dataframe = df,\n",
    "    x_col = 'file_path',\n",
    "    y_col = 'Label',\n",
    "    target_size = (224,224),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "\u001b[1m43941136/43941136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB3(\n",
    "    include_top= False, \n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3), \n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(rate=.45, seed=123),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    Adamax(learning_rate= .0001), \n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'acc', \n",
    "        tf.keras.metrics.Precision(), \n",
    "        tf.keras.metrics.Recall(), \n",
    "        tf.keras.metrics.AUC()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0,\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    restore_best_weights=True,\n",
    "    baseline=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cohort23/Library/Python/3.10/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 3s/step - acc: 0.7257 - auc: 0.9273 - loss: 0.7690 - precision: 0.7958 - recall: 0.6611 - val_acc: 0.1362 - val_auc: 0.4900 - val_loss: 3.2584 - val_precision: 0.1417 - val_recall: 0.1172\n",
      "Epoch 2/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 3s/step - acc: 0.8306 - auc: 0.9720 - loss: 0.4653 - precision: 0.8689 - recall: 0.7981 - val_acc: 0.1267 - val_auc: 0.4618 - val_loss: 3.9890 - val_precision: 0.1302 - val_recall: 0.1158\n",
      "Epoch 3/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - acc: 0.8740 - auc: 0.9834 - loss: 0.3598 - precision: 0.9025 - recall: 0.8483 - val_acc: 0.1213 - val_auc: 0.4389 - val_loss: 4.5112 - val_precision: 0.1248 - val_recall: 0.1144\n",
      "Epoch 4/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 3s/step - acc: 0.9085 - auc: 0.9893 - loss: 0.2793 - precision: 0.9258 - recall: 0.8864 - val_acc: 0.1213 - val_auc: 0.4236 - val_loss: 4.8587 - val_precision: 0.1250 - val_recall: 0.1172\n",
      "Epoch 5/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 3s/step - acc: 0.9342 - auc: 0.9945 - loss: 0.1933 - precision: 0.9510 - recall: 0.9179 - val_acc: 0.1172 - val_auc: 0.4204 - val_loss: 5.2333 - val_precision: 0.1210 - val_recall: 0.1144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a45869e0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data = valid_gen,\n",
    "    epochs = 5,\n",
    "    validation_steps=None,\n",
    "    shuffle=False,\n",
    "    callbacks = early_stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flowers.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=flowwers.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39;49m\u001b[39mflowwers.keras\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m input_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mflower_photos/daisy/10791227_7168491604.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m input_image_resize \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(input_image, (\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/src/saving/saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_h5_format\u001b[39m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[39m=\u001b[39mcustom_objects, \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mstr\u001b[39m(filepath)\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.keras\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile not found: filepath=\u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mzip file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile format not supported: filepath=\u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmight have a different name).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: filepath=flowwers.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.saving import load_model\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('flowwers.keras')\n",
    "\n",
    "input_image = cv2.imread('flower_photos/daisy/10791227_7168491604.jpg')\n",
    "\n",
    "input_image_resize = cv2.resize(input_image, (224,224))\n",
    "\n",
    "input_image_scaled = input_image_resize/225\n",
    "image_reshaped = np.reshape(input_image_scaled, [1,224,224,3])\n",
    "\n",
    "input_prediction = model.predict(image_reshaped)\n",
    "print(\"Your flower is a \",np.argmax(input_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
