{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 2.19Git/s]                                                     \n",
      "Fetching encoder.json: 1.05Mit [00:00, 2.93Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 8.39Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:52, 4.42Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 5.30Git/s]                                               \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.17Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 5.61Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "#gpt2.download_gpt2(model_name='124M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "session=gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(session, model_name ='124M', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMERICA YAUN (13), as well as SARIYA (15) and MANDRIDO (16).\n",
      "\n",
      "The team also announced that for the first time, they will be available as an online platform for all Chinese viewers.\n",
      "\n",
      "The structure of the Chinese channel, which has been in operation since 2012, is that the Chinese viewers will receive a single channel that they can watch on their own or in app. The Chinese viewers will be able to watch the entire series on their own devices.\n",
      "\n",
      "According to the official announcement, channels will be available for all Chinese viewers in the future.\n",
      "\n",
      "Fans will also be able to watch the entire series on their iPhones, Android, Windows, Mac or Linux platforms.\n",
      "\n",
      "The purpose of Chinese channels is to provide Chinese viewers with an integrated experience for watching Chinese TV. The first Chinese TV series, SARIYA, was launched in the US in 2009. In addition, SARIYA brought some serious technical advancements in the market.\n",
      "\n",
      "The Chinese television viewers have been pleasantly surprised by the success of SARIYA.\n",
      "\n",
      "According to a statement issued by the Chinese broadcasting organization of the local cultural affairs bureau of the Cultural Ministry of China, the series will create the \"second most complete Chinese television series in the world.\"\n",
      "\n",
      "The Chinese TV viewers will be able to watch the entire series on their own devices.\n",
      "\n",
      "Other news regarding SARIYA's success is that the series will include an official and exclusive English-language version of the series.\n",
      "\n",
      "The series will be produced by the Chinese television company SARIYA. It will have the following features:\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original original storyline written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "The series will be produced by SARIYA. It will have the following features:\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program.\n",
      "\n",
      "An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achieved Achievement Program. An original story written by SARIYA and edited by SARIYA's Achie\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(session, model_name = '124M', prefix= 'AMERICA YA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Shakespeare GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732547121.237565 15755677 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 338025 tokens\n",
      "Training...\n",
      "[1 | 18.90] loss=4.10 avg=4.10\n",
      "[2 | 38.44] loss=3.80 avg=3.95\n",
      "[3 | 57.48] loss=3.76 avg=3.89\n",
      "[4 | 76.48] loss=3.60 avg=3.81\n",
      "[5 | 95.20] loss=3.55 avg=3.76\n",
      "[6 | 113.85] loss=3.63 avg=3.74\n",
      "[7 | 132.39] loss=3.63 avg=3.72\n",
      "[8 | 150.74] loss=3.54 avg=3.70\n",
      "[9 | 169.39] loss=3.56 avg=3.68\n",
      "[10 | 188.22] loss=3.43 avg=3.66\n",
      "[11 | 207.07] loss=3.72 avg=3.66\n",
      "[12 | 226.18] loss=3.49 avg=3.65\n",
      "[13 | 246.03] loss=3.76 avg=3.66\n",
      "[14 | 264.45] loss=3.35 avg=3.63\n",
      "[15 | 283.07] loss=3.58 avg=3.63\n",
      "[16 | 301.78] loss=3.52 avg=3.62\n",
      "[17 | 320.74] loss=3.50 avg=3.61\n",
      "[18 | 338.82] loss=3.71 avg=3.62\n",
      "[19 | 357.68] loss=3.61 avg=3.62\n",
      "[20 | 376.68] loss=3.40 avg=3.61\n",
      "[21 | 395.43] loss=3.45 avg=3.60\n",
      "[22 | 414.44] loss=3.64 avg=3.60\n",
      "[23 | 433.49] loss=3.37 avg=3.59\n",
      "[24 | 452.74] loss=3.63 avg=3.59\n",
      "[25 | 472.08] loss=3.25 avg=3.58\n",
      "[26 | 490.99] loss=3.14 avg=3.56\n",
      "[27 | 510.20] loss=3.34 avg=3.55\n",
      "[28 | 529.06] loss=3.39 avg=3.54\n",
      "[29 | 548.15] loss=3.32 avg=3.53\n",
      "[30 | 567.15] loss=3.36 avg=3.53\n",
      "[31 | 586.17] loss=3.42 avg=3.52\n",
      "[32 | 605.26] loss=3.50 avg=3.52\n",
      "[33 | 624.12] loss=3.16 avg=3.51\n",
      "[34 | 643.27] loss=3.24 avg=3.50\n",
      "[35 | 662.22] loss=3.29 avg=3.49\n",
      "[36 | 680.92] loss=3.48 avg=3.49\n",
      "[37 | 699.83] loss=3.40 avg=3.49\n",
      "[38 | 718.70] loss=2.95 avg=3.47\n",
      "[39 | 740.41] loss=3.50 avg=3.47\n",
      "[40 | 759.56] loss=3.12 avg=3.46\n",
      "[41 | 778.49] loss=3.26 avg=3.46\n",
      "[42 | 797.95] loss=3.30 avg=3.45\n",
      "[43 | 819.00] loss=3.42 avg=3.45\n",
      "[44 | 838.45] loss=3.32 avg=3.45\n",
      "[45 | 857.43] loss=3.50 avg=3.45\n",
      "[46 | 876.23] loss=3.30 avg=3.44\n",
      "[47 | 895.19] loss=3.24 avg=3.44\n",
      "[48 | 913.99] loss=3.44 avg=3.44\n",
      "[49 | 932.83] loss=3.19 avg=3.43\n",
      "[50 | 951.54] loss=2.98 avg=3.42\n",
      "[51 | 970.03] loss=3.55 avg=3.42\n",
      "[52 | 988.46] loss=3.36 avg=3.42\n",
      "[53 | 1007.23] loss=3.13 avg=3.42\n",
      "[54 | 1025.76] loss=3.67 avg=3.42\n",
      "[55 | 1044.06] loss=3.17 avg=3.42\n",
      "[56 | 1062.35] loss=3.31 avg=3.41\n",
      "[57 | 1080.50] loss=3.12 avg=3.41\n",
      "[58 | 1099.21] loss=3.26 avg=3.40\n",
      "[59 | 1117.47] loss=3.32 avg=3.40\n",
      "[60 | 1136.12] loss=3.27 avg=3.40\n",
      "[61 | 1155.05] loss=3.17 avg=3.39\n",
      "[62 | 1173.10] loss=3.48 avg=3.40\n",
      "[63 | 1191.92] loss=3.26 avg=3.39\n",
      "[64 | 1210.59] loss=3.42 avg=3.39\n",
      "[65 | 1229.61] loss=3.19 avg=3.39\n",
      "[66 | 1248.46] loss=3.17 avg=3.38\n",
      "[67 | 1267.97] loss=3.35 avg=3.38\n",
      "[68 | 1287.10] loss=3.30 avg=3.38\n",
      "[69 | 1306.77] loss=3.16 avg=3.38\n",
      "[70 | 1326.94] loss=3.54 avg=3.38\n",
      "[71 | 1346.24] loss=3.04 avg=3.37\n",
      "[72 | 1365.30] loss=3.34 avg=3.37\n",
      "[73 | 1384.64] loss=3.15 avg=3.37\n",
      "[74 | 1405.33] loss=3.07 avg=3.36\n",
      "[75 | 1425.65] loss=3.28 avg=3.36\n",
      "[76 | 1444.67] loss=3.26 avg=3.36\n",
      "[77 | 1463.48] loss=3.10 avg=3.35\n",
      "[78 | 1482.33] loss=3.52 avg=3.36\n",
      "[79 | 1501.30] loss=3.36 avg=3.36\n",
      "[80 | 1520.29] loss=3.20 avg=3.36\n",
      "[81 | 1539.21] loss=3.30 avg=3.35\n",
      "[82 | 1558.52] loss=3.19 avg=3.35\n",
      "[83 | 1577.71] loss=2.90 avg=3.34\n",
      "[84 | 1596.52] loss=3.31 avg=3.34\n",
      "[85 | 1615.19] loss=3.29 avg=3.34\n",
      "[86 | 1634.04] loss=3.20 avg=3.34\n",
      "[87 | 1653.28] loss=3.13 avg=3.34\n",
      "[88 | 1672.81] loss=3.18 avg=3.33\n",
      "[89 | 1691.72] loss=3.04 avg=3.33\n",
      "[90 | 1711.23] loss=3.29 avg=3.33\n",
      "[91 | 1731.27] loss=3.37 avg=3.33\n",
      "[92 | 1750.98] loss=3.20 avg=3.33\n",
      "[93 | 1770.97] loss=3.19 avg=3.32\n",
      "[94 | 1790.82] loss=3.03 avg=3.32\n",
      "[95 | 1810.47] loss=3.28 avg=3.32\n",
      "[96 | 1829.91] loss=3.14 avg=3.32\n",
      "[97 | 1849.80] loss=3.09 avg=3.31\n",
      "[98 | 1869.30] loss=3.16 avg=3.31\n",
      "[99 | 1889.12] loss=2.99 avg=3.30\n",
      "[100 | 1908.85] loss=3.26 avg=3.30\n",
      "Saving checkpoint/shakespear/model-100\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    session2,\n",
    "    'shakespear.txt',\n",
    "    model_name='124M',\n",
    "    steps=100,\n",
    "    run_name='shakespear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christan: rawr\n",
      "Who, by the winter's day, hath made the night's night\n",
      "Like a mighty mountain forts?\n",
      "\n",
      "HASTINGS:\n",
      "Trey, it is a view I'll have had for thee:\n",
      "You will have seen that I spoke of thee,\n",
      "And thou, my fair queen.\n",
      "\n",
      "HASTINGS:\n",
      "Poor, poor fool!\n",
      "\n",
      "KING HENRY VI:\n",
      "Come, let's hear it.\n",
      "\n",
      "JULIET:\n",
      "I'll speak it,\n",
      "When thou wilt;\n",
      "As I hear it, O, and thou't;\n",
      "As thou knowst:\n",
      "As thou knowst, I am the queen of this\n",
      "As I can think, to a thing\n",
      "Or to a queen, as thou mayst think\n",
      "A queen, that is not a queen.\n",
      "\n",
      "KING HENRY VI:\n",
      "I'll have thee know it,\n",
      "And thou't shalt, and thou't shalt,\n",
      "With my death; and thou't, and thou're not,\n",
      "As thou knowest;\n",
      "As thou knowest, and thou't, as thou knowest.\n",
      "\n",
      "JULIET:\n",
      "I thank the gods, and it is the\n",
      "Tertullian's fault that thou or thy\n",
      "persuasion should make him lose. Thou\n",
      "mayst have made it worse, if thou shouldst do it\n",
      "Since he had been queen; for he hath made\n",
      "the queen what he would have done with her.\n",
      "\n",
      "KING HENRY VI:\n",
      "Why, he's upon our side; and he'll\n",
      "I would have been queen.\n",
      "\n",
      "JULIET:\n",
      "My lord.\n",
      "\n",
      "CATESBY:\n",
      "O, if thou wilt,\n",
      "Thou shalt have fault with me, and thou'rt\n",
      "me.\n",
      "\n",
      "KING HENRY VI:\n",
      "Thou'rt fault, and thou'rt me.\n",
      "\n",
      "JULIET:\n",
      "I will not, nor thou'rt me.\n",
      "\n",
      "CATESBY:\n",
      "I will not, nor thou'rt me.\n",
      "\n",
      "KING HENRY VI:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "JULIET:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "KING HENRY VI:\n",
      "I will not, nor thou'rt me.\n",
      "\n",
      "CATESBY:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "KING HENRY VI:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "JULIET:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "CATESBY:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "KING HENRY VI:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "JULIET:\n",
      "I will not, nor thou'rt me.\n",
      "\n",
      "CATESBY:\n",
      "I'll not, nor thou'rt me.\n",
      "\n",
      "KING HENRY VI:\n",
      "Pray, my lord, I know thou wilt.\n",
      "\n",
      "CATESBY:\n",
      "I know thee well, and thou'rt not.\n",
      "\n",
      "KING HENRY VI:\n",
      "I know thee well, and thou'rt not.\n",
      "\n",
      "JULIET:\n",
      "I know thee well, and thou'rt not.\n",
      "\n",
      "CATESBY:\n",
      "I know thee well, and thou'rt not.\n",
      "\n",
      "KING HENRY VI:\n",
      "I have a power to speak,\n",
      "And thou must, and thou must not,\n",
      "But thou'rt not; and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n",
      "And thou'rt not, and thou'rt not, and thou'rt not,\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session2,\n",
    "    prefix='Christan: rawr',\n",
    "    run_name='shakespear'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
